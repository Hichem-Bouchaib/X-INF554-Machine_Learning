{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Introduction to deep learning<center>\n",
    "    \n",
    "## Data\n",
    "\n",
    "For this lab we are going to use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.\n",
    "This very standard dataset is widely used as a baseline in deep learning. Both Keras and Pytorch have datasets APIs containing this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Vizualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    YOURE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first model\n",
    "\n",
    "We are going to use Keras' Sequential API\n",
    "\n",
    "First we need to define the layers in a list:\n",
    "```python\n",
    "layers = [\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "]\n",
    "```\n",
    "<p>The `Flatten` layer transforms the matrix into a features vector.</p>\n",
    "`Dense` layers (also known as fully connected) are classical matrix multiplications.\n",
    "\n",
    "Then the model is defined as the sequence of layers, and compiled to define the loss and the optimizer\n",
    "```python \n",
    "model = keras.Sequential(layers)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Finally we can train the network on the training dataset while testing on the test set\n",
    "```python\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test))\n",
    "```\n",
    "\n",
    "`history` stores the training curves.\n",
    "\n",
    "### Task 2 - Implement and train the model on Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the training curve to evaluate the quality of the training.\n",
    "\n",
    "### Task 3 - Complete the `plot_history` to plot the train and test losses and accuracies curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation of the Units\n",
    "\n",
    "To understand how a neural network interprete the data, we can visualize activation of the units.\n",
    "In `Keras` a model can be defined only be specifying the input and the output if the weights to link them already exist. This is usefull to acces the inner layers of a network.\n",
    "```ptyhon\n",
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                 outputs=model.layers[1].output)\n",
    "```\n",
    "The previous model takes as input an image and returns the activated units of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights matrices\n",
    "\n",
    "### Task 4 - Vizualize the weights connecting every pixel of the image to each units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional network\n",
    "\n",
    "With images, convolutinonal layers are more efficient than Dense layers. We need to add a 4th dimension to the input data to pass it to a convolution layer. This correspond to the channel, a standard RGB image as 3, but our gray image only has one.\n",
    "\n",
    "Two new layers:\n",
    "```python\n",
    "keras.layers.Conv2D(8, kernel_size=3, activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=2, padding=\"valid\"),\n",
    "```\n",
    "The convolution applies the convolution filters and the maxpooling layers aggregates the feature maps on squared regions.\n",
    "\n",
    "### Task 5 - Implement a convolutional neural network with 2 convolutional layers and train it on MNIST.\n",
    "\n",
    "#### Note :\n",
    "Tensorflow does not dealocate the weights of a network, even if you delete the model. To completely clear the weights of a model you need to call `keras.backend.clear_session()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 - vizualize the weights of the first convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 - Using intermediate models, vizualize the outputs of the covolutions and max pooling for each layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in range(2):\n",
    "    conv = keras.Model(inputs=model.input, outputs=model.layers[layer*2+1].output)\n",
    "    pool = keras.Model(inputs=model.input, outputs=model.layers[layer*2+2].output)\n",
    "    units = conv.predict(x_test)\n",
    "    pooled = pool.predict(x_test)\n",
    "\n",
    "    fig, axs = plt.subplots(1,8, figsize=(40,5))\n",
    "    for _ in range(8):\n",
    "        axs[_].imshow(np.array(units[1,:,:,_]), cmap=\"gray\")\n",
    "        axs[_].set_title(\"Laer %i - Unit map %i\" % (layer, _+1))\n",
    "        \n",
    "    fig, axs = plt.subplots(1,8,figsize=(40,5))\n",
    "    for _ in range(8):\n",
    "        axs[_].imshow(np.array(pooled[1,:,:,_]), cmap=\"gray\")\n",
    "        axs[_].set_title(\"Laer %i - Unit map after pooling %i\" % (layer, _+1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better optimization and Tensorboard\n",
    "\n",
    "Up to now, we've let asside an important subject of deep learning, hyper-parameters tunning.\n",
    "\n",
    "You can define an optimizer with the `optimizer` module of `Keras`:\n",
    "\n",
    "```python\n",
    "keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
    "keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
    "keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "```\n",
    "\n",
    "One of the most usefull features of `tensorflow`, `tensorboard`, is extremely easy to set up in `Keras`.\n",
    "```python\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "```\n",
    "\n",
    "Another common practice is to decrease the learning rate when the network stops learning.\n",
    "This can be achierved with a callback:\n",
    "```python\n",
    "lrate = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)\n",
    "```\n",
    "\n",
    "### Task 8 - Play with those optimizers to get more suitable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now visualize your network and metrics with tensorboard. If you dont have it install it with ``` pip install tensorboard\n",
    "``` and then run it with ```tensorboard --logdir logs```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Inf554",
   "language": "python",
   "name": "inf554"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
